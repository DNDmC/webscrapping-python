# WebScraping com Python

Um projeto completo de anÃ¡lise de mercado que coleta, processa e visualiza dados sobre notebooks no Mercado Livre Brasil.

## Problema de NegÃ³cio

A Samsung necessita de uma pesquisa de mercado aprofundada na categoria de notebooks para tomar decisÃµes estratÃ©gicas, como ela se compara aos concorrentes dentro de MarketPlaces, para esse feito ela decidiu que precisa coletar dados,
meu papel Ã© realizar uma coleta desses dados nesses sites, neste caso optei pelos dados do MarketPlace do MercadoLivre, como todo projeto, precisamos entender o que o cleinte deseja
nesse caso A empresa deseja:

* Realizar uma anÃ¡lise completa da concorrÃªncia de notebooks em marketplaces, especificamente no Mercado Livre
* Coletar dados estruturados (Nome, Marca, PreÃ§o, AvaliaÃ§Ãµes, Vendedor, etc.)
* Desenvolver um sistema de WebScraping para obtenÃ§Ã£o automatizada dos dados
* Armazenar as informaÃ§Ãµes em um banco de dados SQL para consultas futuras
* Criar um dashboard interativo para visualizaÃ§Ã£o e anÃ¡lise dos insights obtidos

Uma vez compreendido o desejo da empresa e o problema de negÃ³cio podemos prosseguir com a explicaÃ§Ã£o de todo o processo feito para conseguir tais informaÃ§Ãµes.

Este projeto Ã© uma soluÃ§Ã£o end-to-end de webscraping, seguindo o formato Extract, Transform e Load (ETL).

## Processo de Extract (ExtraÃ§Ã£o)

Utilizei o framework Scrapy para webscraping. O spider `notebook.py` foi programado para navegar pela pÃ¡gina de notebooks do Mercado Livre e extrair informaÃ§Ãµes detalhadas.

Um dos maiores desafios foi compreender o padrÃ£o de URLs do Mercado Livre. Identifiquei que a navegaÃ§Ã£o seguia um padrÃ£o especÃ­fico:
- Para a segunda pÃ¡gina: `.../notebook_Desde_49_NoIndex_True`
- Para as pÃ¡ginas seguintes: `.../notebook_Desde_{49+(pÃ¡gina-1)*48}_NoIndex_True`

Este padrÃ£o estÃ¡ implementado no bloco condicional ao final do spider, permitindo a navegaÃ§Ã£o automÃ¡tica entre as pÃ¡ginas:

```python
if self.page_count < self.max_page:
    # Verificamos se o contador de pÃ¡ginas estÃ¡ abaixo do limite mÃ¡ximo definido (10 pÃ¡ginas)
    if self.page_count == 1:
        # Tratamento especial para a segunda pÃ¡gina (depois da primeira)
        # A segunda pÃ¡gina sempre usa um offset fixo de 49 produtos
        next_page = "https://lista.mercadolivre.com.br/informatica/portateis-acessorios/notebooks/notebook_Desde_49_NoIndex_True"
    else:
        # Para as pÃ¡ginas subsequentes (terceira em diante)
        # Calculamos o offset com base em uma fÃ³rmula identificada no padrÃ£o do site:
        # - Primeira pÃ¡gina: sem offset
        # - Segunda pÃ¡gina: offset de 49
        # - Terceira pÃ¡gina em diante: 49 + (nÃºmero_da_pÃ¡gina - 1) * 48
        next_offset = 49 + (self.page_count - 1) * 48
        next_page = f"https://lista.mercadolivre.com.br/informatica/portateis-acessorios/notebooks/notebook_Desde_{next_offset}_NoIndex_True"
    
    # Incrementamos o contador de pÃ¡ginas para a prÃ³xima iteraÃ§Ã£o
    self.page_count += 1
    
    # Criamos uma nova requisiÃ§Ã£o para a prÃ³xima pÃ¡gina e a adicionamos Ã  fila do Scrapy
    # O callback self.parse faz com que a mesma funÃ§Ã£o seja chamada para processar os resultados
    yield scrapy.Request(url=next_page, callback=self.parse)
```

Este sistema funciona porque:
1. Inicializamos `page_count = 1` e `max_page = 10` no inÃ­cio da classe
2. A condiÃ§Ã£o `if self.page_count < self.max_page` limita a coleta a no mÃ¡ximo 10 pÃ¡ginas
3. Tratamos a primeira transiÃ§Ã£o (pÃ¡gina 1 â†’ pÃ¡gina 2) de forma especial, pois o Mercado Livre usa um offset fixo de 49
4. Para as prÃ³ximas pÃ¡ginas, calculamos o offset dinamicamente baseado no padrÃ£o descoberto
5. Incrementamos o contador apÃ³s cada pÃ¡gina processada
6. Usamos o `yield` para gerar uma nova requisiÃ§Ã£o que serÃ¡ processada pelo Scrapy

Outro aspecto importante foi a extraÃ§Ã£o de preÃ§os. O Mercado Livre frequentemente exibe dois valores: o preÃ§o original (`old_money`) e o preÃ§o promocional (`new_money`). ExtraÃ­ esses valores usando:

```python
prices = product.css('span.andes-money-amount__fraction::text').getall()
'old_money': prices[0] if len(prices) > 0 else None,
'new_money': prices[1] if len(prices) > 1 else None
```

### Processo de Transform (TransformaÃ§Ã£o)

A transformaÃ§Ã£o dos dados foi realizada com a biblioteca Pandas. As principais operaÃ§Ãµes incluÃ­ram:

1. **Tratamento de dados nulos**:
   ```python
   df['old_money'] = df['old_money'].fillna('0')
   df['new_money'] = df['new_money'].fillna('0')
   df['reviews_rating_number'] = df['reviews_rating_number'].fillna('0')
   df['reviews_amount'] = df['reviews_amount'].fillna('0')
   ```

2. **Filtragem de preÃ§os** - mantendo apenas notebooks entre R$1.000 e R$10.000:
   ```python
   df = df[
       (df['old_money'] >= 1000) & (df['old_money'] <= 10000) &
       (df['new_money'] >= 1000) & (df['new_money'] <= 10000)
   ]
   ```

3. **ConversÃ£o de tipos de dados**:
   ```python
   df['old_money'] = df['old_money'].astype(float)
   df['new_money'] = df['new_money'].astype(float)
   df['reviews_rating_number'] = df['reviews_rating_number'].astype(float)
   df['reviews_amount'] = df['reviews_amount'].astype(int)
   ```

### Processo de Load (Carregamento)

O carregamento dos dados foi feito com SQLite:

```python
conn = sqlite3.connect('data/meli.sql')
df.to_sql('notebook', conn, if_exists='replace', index=False)
```

#### Teste de ConexÃ£o com DBeaver

ApÃ³s o carregamento dos dados, realizei testes de conexÃ£o utilizando o DBeaver para verificar se o banco de dados foi criado corretamente e se a conexÃ£o poderia ser estabelecida.

![Teste de ConexÃ£o com DBeaver](images/teste_conexao.png)
*Figura 1: ConfiguraÃ§Ã£o da conexÃ£o com o banco de dados SQLite no DBeaver.*

#### Checagem dos Dados no DBeaver

Para validar a integridade dos dados carregados, executei uma consulta simples no DBeaver:

```sql
SELECT * FROM notebook
```

Esta consulta retornou todos os registros da tabela, permitindo verificar se os dados foram corretamente carregados e estruturados.

![Checagem dos Dados no DBeaver](images/checagem_dados.png)
*Figura 2: Resultados da consulta SQL exibindo os dados coletados dos notebooks.*

### Dashboard com Streamlit

Desenvolvi um dashboard interativo com Streamlit que apresenta:
- KPIs principais: total de notebooks, marcas Ãºnicas e preÃ§o mÃ©dio
- GrÃ¡ficos de barras das marcas mais frequentes
- AnÃ¡lise de preÃ§o mÃ©dio por marca
- SatisfaÃ§Ã£o mÃ©dia por marca baseada nas avaliaÃ§Ãµes

#### CÃ³digo do Dashboard

```python
# import
import streamlit as st
import pandas as pd
import sqlite3

# Conectar ao banco de dados SQLite
conn = sqlite3.connect('data/meli.sql')

# Carregar os dados da tabela 'notebooks' em um DataFrame pandas
df = pd.read_sql_query("SELECT * FROM notebook", conn)

# Fechar a conexÃ£o com o banco de dados
conn.close()

# TÃ­tulo da aplicaÃ§Ã£o
st.title('ğŸ“Š Pesquisa de Mercado - Notebooks no Mercado Livre')

# Melhorar o layout com colunas para KPIs
st.subheader('ğŸ’¡ KPIs principais')
col1, col2, col3 = st.columns(3)

# KPI 1: NÃºmero total de itens
total_itens = df.shape[0]
col1.metric(label="ğŸ–¥ï¸ Total de Notebooks", value=total_itens)

# KPI 2: NÃºmero de marcas Ãºnicas
unique_brands = df['brand'].nunique()
col2.metric(label="ğŸ·ï¸ Marcas Ãšnicas", value=unique_brands)

# KPI 3: PreÃ§o mÃ©dio novo (em reais)
average_new_price = df['new_money'].mean()
col3.metric(label="ğŸ’° PreÃ§o MÃ©dio (R$)", value=f"{average_new_price:.2f}")

# Marcas mais frequentes
st.subheader('ğŸ† Marcas mais encontradas atÃ© a 10Âª pÃ¡gina')
col1, col2 = st.columns([4, 2])
top_brands = df['brand'].value_counts().sort_values(ascending=False)
col1.bar_chart(top_brands)
col2.write(top_brands)

# PreÃ§o mÃ©dio por marca
st.subheader('ğŸ’µ PreÃ§o mÃ©dio por marca')
col1, col2 = st.columns([4, 2])
df_non_zero_prices = df[df['new_money'] > 0]
average_price_by_brand = df_non_zero_prices.groupby('brand')['new_money'].mean().sort_values(ascending=False)
col1.bar_chart(average_price_by_brand)
col2.write(average_price_by_brand)

# SatisfaÃ§Ã£o mÃ©dia por marca
st.subheader('â­ SatisfaÃ§Ã£o mÃ©dia por marca')
col1, col2 = st.columns([4, 2])
df_non_zero_reviews = df[df['reviews_rating_number'] > 0]
satisfaction_by_brand = df_non_zero_reviews.groupby('brand')['reviews_rating_number'].mean().sort_values(ascending=False)
col1.bar_chart(satisfaction_by_brand)
col2.write(satisfaction_by_brand)
```

#### VisualizaÃ§Ã£o do Dashboard

Abaixo estÃ¡ uma captura de tela do dashboard em funcionamento:

![Dashboard Streamlit](images/dashboard_streamlit.png)
*Figura 3: Dashboard interativo criado com Streamlit mostrando os principais insights dos dados coletados.*

## Estrutura do Projeto

```
ğŸ“¦
â”œâ”€â”€ data                 # ContÃ©m os dados coletados e o banco SQLite
â”‚   â””â”€â”€ data.jsonl       # Dados extraÃ­dos em formato JSONL
â”‚   â””â”€â”€ meli.sql         # Banco de dados SQLite com dados processados
â”œâ”€â”€ src                  # CÃ³digo-fonte do projeto
â”‚   â”œâ”€â”€ extraction       # Scripts de coleta de dados (Scrapy)
â”‚   â”‚   â””â”€â”€ coleta       # Projeto Scrapy
â”‚   â”‚       â”œâ”€â”€ spiders  # Spiders para extraÃ§Ã£o de dados
â”‚   â”‚       â”‚   â””â”€â”€ notebook.py # Spider especÃ­fico para notebooks
â”‚   â”‚       â”œâ”€â”€ items.py        # DefiniÃ§Ã£o dos itens a serem coletados
â”‚   â”‚       â””â”€â”€ settings.py     # ConfiguraÃ§Ãµes do Scrapy
â”‚   â”œâ”€â”€ transformLoad    # Scripts de processamento e carregamento
â”‚   â”‚   â””â”€â”€ main.py      # Script de ETL para tratar os dados coletados
â”‚   â””â”€â”€ dashboard        # CÃ³digo da interface de visualizaÃ§Ã£o
â”‚       â””â”€â”€ app.py       # AplicaÃ§Ã£o Streamlit para visualizaÃ§Ã£o dos dados
â””â”€â”€ README.md            # Este arquivo
```

## Como Usar

### PrÃ©-requisitos

- Python 3.8+
- pip (gerenciador de pacotes do Python)
- Bibliotecas: Scrapy, Pandas, SQLite3, Streamlit

### InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/meli-market-research.git
cd meli-market-research

# Instale as dependÃªncias
pip install -r requirements.txt
```

### Executando o Projeto

#### 1. ExtraÃ§Ã£o de Dados (Coleta)

```bash
cd src/extraction
scrapy crawl notebook -o ../../data/data.jsonl
```

Este comando inicia o spider que coleta informaÃ§Ãµes de atÃ© 10 pÃ¡ginas de resultados de notebooks no Mercado Livre.

#### 2. TransformaÃ§Ã£o e Carregamento dos Dados

```bash
cd ../transformLoad
python main.py
```

Este script limpa os dados, remove valores invÃ¡lidos, formata os preÃ§os corretamente e carrega tudo em um banco de dados SQLite.

#### 3. VisualizaÃ§Ã£o dos Dados

```bash
cd ../dashboard
streamlit run app.py
```

Este comando inicia o dashboard Streamlit que exibe graficamente os insights extraÃ­dos dos dados.

## Insights DisponÃ­veis

O dashboard fornece as seguintes informaÃ§Ãµes:

- Total de notebooks analisados
- NÃºmero de marcas disponÃ­veis
- PreÃ§o mÃ©dio dos notebooks
- Ranking das marcas mais populares
- ComparaÃ§Ã£o de preÃ§os mÃ©dios por marca
- SatisfaÃ§Ã£o dos clientes por marca (avaliaÃ§Ãµes)

## Tecnologias Utilizadas

- **Scrapy**: Framework para extraÃ§Ã£o de dados
- **Pandas**: Biblioteca para manipulaÃ§Ã£o e anÃ¡lise de dados
- **SQLite**: Sistema de gerenciamento de banco de dados relacional
- **DBeaver**: Ferramenta universal de banco de dados para checagem dos dados e teste de conexÃ£o 
- **Streamlit**: Biblioteca para criaÃ§Ã£o de aplicaÃ§Ãµes web interativas

## PossÃ­veis Melhorias Futuras

- Implementar coleta automÃ¡tica agendada
- Adicionar anÃ¡lise de sentimento das avaliaÃ§Ãµes
- Incluir comparaÃ§Ã£o histÃ³rica de preÃ§os
- Expandir para outras categorias de produtos
- Implementar alertas de preÃ§o

## LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT - veja o arquivo LICENSE para detalhes.

## Autor

Filipe Albuquerque - [filipi_98@hotmail.com](mailto:filipi_98@hotmail.com)

---

â­ï¸ Se este projeto te ajudou, considere dar uma estrela! â­ï¸
