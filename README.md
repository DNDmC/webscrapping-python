# WebScraping com Python

Um projeto completo de an√°lise de mercado que coleta, processa e visualiza dados sobre notebooks no Mercado Livre Brasil.

## Problema de Neg√≥cio

A Samsung necessita de uma pesquisa de mercado aprofundada na categoria de notebooks para tomar decis√µes estrat√©gicas, como ela se compara aos concorrentes dentro de MarketPlaces, para esse feito ela decidiu que precisa coletar dados,
meu papel √© realizar uma coleta desses dados nesses sites, neste caso optei pelos dados do MarketPlace do MercadoLivre, como todo projeto, precisamos entender o que o cleinte deseja
nesse caso A empresa deseja:

* Realizar uma an√°lise completa da concorr√™ncia de notebooks em marketplaces, especificamente no Mercado Livre
* Coletar dados estruturados (Nome, Marca, Pre√ßo, Avalia√ß√µes, Vendedor, etc.)
* Desenvolver um sistema de WebScraping para obten√ß√£o automatizada dos dados
* Armazenar as informa√ß√µes em um banco de dados SQL para consultas futuras
* Criar um dashboard interativo para visualiza√ß√£o e an√°lise dos insights obtidos

Uma vez compreendido o desejo da empresa e o problema de neg√≥cio podemos prosseguir com a explica√ß√£o de todo o processo feito para conseguir tais informa√ß√µes.

Este projeto √© uma solu√ß√£o end-to-end de webscraping, seguindo o formato Extract, Transform e Load (ETL).

## Processo de Extract (Extra√ß√£o)

Utilizei o framework Scrapy para webscraping. O spider `notebook.py` foi programado para navegar pela p√°gina de notebooks do Mercado Livre e extrair informa√ß√µes detalhadas.

Um dos maiores desafios foi compreender o padr√£o de URLs do Mercado Livre. Identifiquei que a navega√ß√£o seguia um padr√£o espec√≠fico:
- Para a segunda p√°gina: `.../notebook_Desde_49_NoIndex_True`
- Para as p√°ginas seguintes: `.../notebook_Desde_{49+(p√°gina-1)*48}_NoIndex_True`

Este padr√£o est√° implementado no bloco condicional ao final do spider, permitindo a navega√ß√£o autom√°tica entre as p√°ginas:

```python
if self.page_count < self.max_page:
    # Verificamos se o contador de p√°ginas est√° abaixo do limite m√°ximo definido (10 p√°ginas)
    if self.page_count == 1:
        # Tratamento especial para a segunda p√°gina (depois da primeira)
        # A segunda p√°gina sempre usa um offset fixo de 49 produtos
        next_page = "https://lista.mercadolivre.com.br/informatica/portateis-acessorios/notebooks/notebook_Desde_49_NoIndex_True"
    else:
        # Para as p√°ginas subsequentes (terceira em diante)
        # Calculamos o offset com base em uma f√≥rmula identificada no padr√£o do site:
        # - Primeira p√°gina: sem offset
        # - Segunda p√°gina: offset de 49
        # - Terceira p√°gina em diante: 49 + (n√∫mero_da_p√°gina - 1) * 48
        next_offset = 49 + (self.page_count - 1) * 48
        next_page = f"https://lista.mercadolivre.com.br/informatica/portateis-acessorios/notebooks/notebook_Desde_{next_offset}_NoIndex_True"
    
    # Incrementamos o contador de p√°ginas para a pr√≥xima itera√ß√£o
    self.page_count += 1
    
    # Criamos uma nova requisi√ß√£o para a pr√≥xima p√°gina e a adicionamos √† fila do Scrapy
    # O callback self.parse faz com que a mesma fun√ß√£o seja chamada para processar os resultados
    yield scrapy.Request(url=next_page, callback=self.parse)
```

Este sistema funciona porque:
1. Inicializamos `page_count = 1` e `max_page = 10` no in√≠cio da classe
2. A condi√ß√£o `if self.page_count < self.max_page` limita a coleta a no m√°ximo 10 p√°ginas
3. Tratamos a primeira transi√ß√£o (p√°gina 1 ‚Üí p√°gina 2) de forma especial, pois o Mercado Livre usa um offset fixo de 49
4. Para as pr√≥ximas p√°ginas, calculamos o offset dinamicamente baseado no padr√£o descoberto
5. Incrementamos o contador ap√≥s cada p√°gina processada
6. Usamos o `yield` para gerar uma nova requisi√ß√£o que ser√° processada pelo Scrapy

Outro aspecto importante foi a extra√ß√£o de pre√ßos. O Mercado Livre frequentemente exibe dois valores: o pre√ßo original (`old_money`) e o pre√ßo promocional (`new_money`). Extra√≠ esses valores usando:

```python
prices = product.css('span.andes-money-amount__fraction::text').getall()
'old_money': prices[0] if len(prices) > 0 else None,
'new_money': prices[1] if len(prices) > 1 else None
```

### Processo de Transform (Transforma√ß√£o)

A transforma√ß√£o dos dados foi realizada com a biblioteca Pandas. As principais opera√ß√µes inclu√≠ram:

1. **Tratamento de dados nulos**:
   ```python
   df['old_money'] = df['old_money'].fillna('0')
   df['new_money'] = df['new_money'].fillna('0')
   df['reviews_rating_number'] = df['reviews_rating_number'].fillna('0')
   df['reviews_amount'] = df['reviews_amount'].fillna('0')
   ```

2. **Filtragem de pre√ßos** - mantendo apenas notebooks entre R$1.000 e R$10.000:
   ```python
   df = df[
       (df['old_money'] >= 1000) & (df['old_money'] <= 10000) &
       (df['new_money'] >= 1000) & (df['new_money'] <= 10000)
   ]
   ```

3. **Convers√£o de tipos de dados**:
   ```python
   df['old_money'] = df['old_money'].astype(float)
   df['new_money'] = df['new_money'].astype(float)
   df['reviews_rating_number'] = df['reviews_rating_number'].astype(float)
   df['reviews_amount'] = df['reviews_amount'].astype(int)
   ```

### Processo de Load (Carregamento)

O carregamento dos dados foi feito com SQLite:

```python
conn = sqlite3.connect('data/meli.sql')
df.to_sql('notebook', conn, if_exists='replace', index=False)
```

#### Teste de Conex√£o com DBeaver

Ap√≥s o carregamento dos dados, realizei testes de conex√£o utilizando o DBeaver para verificar se o banco de dados foi criado corretamente e se a conex√£o poderia ser estabelecida.

![Teste de Conex√£o com DBeaver](images/teste_conexao.png)

*Figura 1: Configura√ß√£o da conex√£o com o banco de dados SQLite no DBeaver.*

#### Checagem dos Dados no DBeaver

Para validar a integridade dos dados carregados, executei uma consulta simples no DBeaver:

```sql
SELECT * FROM notebook
```

Esta consulta retornou todos os registros da tabela, permitindo verificar se os dados foram corretamente carregados e estruturados.

![Checagem dos Dados no DBeaver](images/checagem_dados.png)

*Figura 2: Resultados da consulta SQL exibindo os dados coletados dos notebooks.*

### Dashboard com Streamlit

Desenvolvi um dashboard interativo com Streamlit que apresenta:
- KPIs principais: total de notebooks, marcas √∫nicas e pre√ßo m√©dio
- Gr√°ficos de barras das marcas mais frequentes
- An√°lise de pre√ßo m√©dio por marca
- Satisfa√ß√£o m√©dia por marca baseada nas avalia√ß√µes

#### C√≥digo do Dashboard

```python
# import
import streamlit as st
import pandas as pd
import sqlite3

# Conectar ao banco de dados SQLite
conn = sqlite3.connect('data/meli.sql')

# Carregar os dados da tabela 'notebooks' em um DataFrame pandas
df = pd.read_sql_query("SELECT * FROM notebook", conn)

# Fechar a conex√£o com o banco de dados
conn.close()

# T√≠tulo da aplica√ß√£o
st.title('üìä Pesquisa de Mercado - Notebooks no Mercado Livre')

# Melhorar o layout com colunas para KPIs
st.subheader('üí° KPIs principais')
col1, col2, col3 = st.columns(3)

# KPI 1: N√∫mero total de itens
total_itens = df.shape[0]
col1.metric(label="üñ•Ô∏è Total de Notebooks", value=total_itens)

# KPI 2: N√∫mero de marcas √∫nicas
unique_brands = df['brand'].nunique()
col2.metric(label="üè∑Ô∏è Marcas √önicas", value=unique_brands)

# KPI 3: Pre√ßo m√©dio novo (em reais)
average_new_price = df['new_money'].mean()
col3.metric(label="üí∞ Pre√ßo M√©dio (R$)", value=f"{average_new_price:.2f}")

# Marcas mais frequentes
st.subheader('üèÜ Marcas mais encontradas at√© a 10¬™ p√°gina')
col1, col2 = st.columns([4, 2])
top_brands = df['brand'].value_counts().sort_values(ascending=False)
col1.bar_chart(top_brands)
col2.write(top_brands)

# Pre√ßo m√©dio por marca
st.subheader('üíµ Pre√ßo m√©dio por marca')
col1, col2 = st.columns([4, 2])
df_non_zero_prices = df[df['new_money'] > 0]
average_price_by_brand = df_non_zero_prices.groupby('brand')['new_money'].mean().sort_values(ascending=False)
col1.bar_chart(average_price_by_brand)
col2.write(average_price_by_brand)

# Satisfa√ß√£o m√©dia por marca
st.subheader('‚≠ê Satisfa√ß√£o m√©dia por marca')
col1, col2 = st.columns([4, 2])
df_non_zero_reviews = df[df['reviews_rating_number'] > 0]
satisfaction_by_brand = df_non_zero_reviews.groupby('brand')['reviews_rating_number'].mean().sort_values(ascending=False)
col1.bar_chart(satisfaction_by_brand)
col2.write(satisfaction_by_brand)
```

#### Visualiza√ß√£o do Dashboard

Abaixo est√° uma captura de tela do dashboard em funcionamento:

![Dashboard Streamlit](images/dashboard_streamlit.png)

*Figura 3: Dashboard interativo criado com Streamlit mostrando os principais insights dos dados coletados.

## üìä Dicion√°rio de Dados

### Tabela: `notebook`

| Campo | Tipo | Descri√ß√£o | Fun√ß√£o |
|-------|------|-----------|--------|
| `brand` | TEXT | Marca do notebook | Permite agrupar e analisar produtos por fabricante |
| `name` | TEXT | Nome/descri√ß√£o do produto | Armazena as especifica√ß√µes t√©cnicas e caracter√≠sticas do produto |
| `seller` | TEXT | Nome do vendedor | Identifica o comerciante respons√°vel pela oferta do produto |
| `reviews_rating_number` | FLOAT | Nota m√©dia das avalia√ß√µes (0-5) | Quantifica a satisfa√ß√£o dos clientes em uma escala padronizada |
| `reviews_amount` | INTEGER | Quantidade de avalia√ß√µes | Indica o volume de feedback e serve como m√©trica de confiabilidade da avalia√ß√£o m√©dia |
| `old_money` | FLOAT | Pre√ßo original (em R$) | Registra o valor de refer√™ncia para c√°lculo de descontos |
| `new_money` | FLOAT | Pre√ßo atual/promocional (em R$) | Armazena o valor efetivo de venda para compara√ß√µes de pre√ßo |
| `_source` | TEXT | URL de origem dos dados | Mant√©m a rastreabilidade da fonte dos dados para verifica√ß√£o e auditoria |
| `_datetime` | DATETIME | Data e hora da coleta | Permite an√°lises temporais e verifica√ß√£o de atualidade dos dados |

## Estrutura do Projeto

```
üì¶
‚îú‚îÄ‚îÄ data                 # Cont√©m os dados coletados e o banco SQLite
‚îÇ   ‚îî‚îÄ‚îÄ data.jsonl       # Dados extra√≠dos em formato JSONL
‚îÇ   ‚îî‚îÄ‚îÄ meli.sql         # Banco de dados SQLite com dados processados
‚îú‚îÄ‚îÄ src                  # C√≥digo-fonte do projeto
‚îÇ   ‚îú‚îÄ‚îÄ extraction       # Scripts de coleta de dados (Scrapy)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ coleta       # Projeto Scrapy
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ spiders  # Spiders para extra√ß√£o de dados
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ notebook.py # Spider espec√≠fico para notebooks
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ items.py        # Defini√ß√£o dos itens a serem coletados
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ settings.py     # Configura√ß√µes do Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ transformLoad    # Scripts de processamento e carregamento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py      # Script de ETL para tratar os dados coletados
‚îÇ   ‚îî‚îÄ‚îÄ dashboard        # C√≥digo da interface de visualiza√ß√£o
‚îÇ       ‚îî‚îÄ‚îÄ app.py       # Aplica√ß√£o Streamlit para visualiza√ß√£o dos dados
‚îî‚îÄ‚îÄ README.md            # Este arquivo
```

## Como Usar

### Pr√©-requisitos

- Python 3.8+
- pip (gerenciador de pacotes do Python)
- Bibliotecas: Scrapy, Pandas, SQLite3, Streamlit

### Instala√ß√£o

```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/meli-market-research.git
cd meli-market-research

# Instale as depend√™ncias
pip install -r requirements.txt
```

### Executando o Projeto

#### 1. Extra√ß√£o de Dados (Coleta)

```bash
cd src/extraction
scrapy crawl notebook -o ../../data/data.jsonl
```

Este comando inicia o spider que coleta informa√ß√µes de at√© 10 p√°ginas de resultados de notebooks no Mercado Livre.

#### 2. Transforma√ß√£o e Carregamento dos Dados

```bash
cd ../transformLoad
python main.py
```

Este script limpa os dados, remove valores inv√°lidos, formata os pre√ßos corretamente e carrega tudo em um banco de dados SQLite.

#### 3. Visualiza√ß√£o dos Dados

```bash
cd ../dashboard
streamlit run app.py
```

Este comando inicia o dashboard Streamlit que exibe graficamente os insights extra√≠dos dos dados.

## Insights Dispon√≠veis

O dashboard fornece as seguintes informa√ß√µes:

- Total de notebooks analisados
- N√∫mero de marcas dispon√≠veis
- Pre√ßo m√©dio dos notebooks
- Ranking das marcas mais populares
- Compara√ß√£o de pre√ßos m√©dios por marca
- Satisfa√ß√£o dos clientes por marca (avalia√ß√µes)

## Tecnologias Utilizadas

- **Scrapy**: Framework para extra√ß√£o de dados
- **Pandas**: Biblioteca para manipula√ß√£o e an√°lise de dados
- **SQLite**: Sistema de gerenciamento de banco de dados relacional
- **DBeaver**: Ferramenta universal de banco de dados para checagem dos dados e teste de conex√£o 
- **Streamlit**: Biblioteca para cria√ß√£o de aplica√ß√µes web interativas

## Poss√≠veis Melhorias Futuras

- Implementar coleta autom√°tica agendada
- Adicionar an√°lise de sentimento das avalia√ß√µes
- Incluir compara√ß√£o hist√≥rica de pre√ßos
- Expandir para outras categorias de produtos
- Implementar alertas de pre√ßo

## Licen√ßa

Este projeto est√° licenciado sob a licen√ßa MIT - veja o arquivo LICENSE para detalhes.

## Autor

Filipe Albuquerque - [filipi_98@hotmail.com](mailto:filipi_98@hotmail.com)

---

‚≠êÔ∏è Se este projeto te ajudou, considere dar uma estrela! ‚≠êÔ∏è
