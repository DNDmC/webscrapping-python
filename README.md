# MELI Notebook Market Research

Um projeto completo de an√°lise de mercado que coleta, processa e visualiza dados sobre notebooks no Mercado Livre Brasil.

## Problema de Neg√≥cio

A Samsung necessita de uma pesquisa de mercado aprofundada na categoria de notebooks para tomar decis√µes estrat√©gicas. A empresa deseja:

* Realizar uma an√°lise completa da concorr√™ncia de notebooks em marketplaces, especificamente no Mercado Livre
* Coletar dados estruturados (Nome, Marca, Pre√ßo, Avalia√ß√µes, Vendedor, etc.)
* Desenvolver um sistema de WebScraping para obten√ß√£o automatizada dos dados
* Armazenar as informa√ß√µes em um banco de dados SQL para consultas futuras
* Criar um dashboard interativo para visualiza√ß√£o e an√°lise dos insights obtidos

Este projeto √© uma solu√ß√£o end-to-end de webscraping, seguindo o formato Extract, Transform e Load (ETL).

### Processo de Extract (Extra√ß√£o)

Utilizei o framework Scrapy para webscraping. O spider `notebook.py` foi programado para navegar pela p√°gina de notebooks do Mercado Livre e extrair informa√ß√µes detalhadas.

Um dos maiores desafios foi compreender o padr√£o de URLs do Mercado Livre. Identifiquei que a navega√ß√£o seguia um padr√£o espec√≠fico:
- Para a segunda p√°gina: `.../notebook_Desde_49_NoIndex_True`
- Para as p√°ginas seguintes: `.../notebook_Desde_{49+(p√°gina-1)*48}_NoIndex_True`

Este padr√£o est√° implementado no bloco condicional ao final do spider, permitindo a navega√ß√£o autom√°tica entre as p√°ginas:

```python
if self.page_count < self.max_page:
    # Construir a URL da pr√≥xima p√°gina com base no padr√£o identificado
    if self.page_count == 1:
        # Para a segunda p√°gina (ap√≥s a primeira)
        next_page = "https://lista.mercadolivre.com.br/informatica/portateis-acessorios/notebooks/notebook_Desde_49_NoIndex_True"
    else:
        # Para as p√°ginas subsequentes
        next_offset = 49 + (self.page_count - 1) * 48
        next_page = f"https://lista.mercadolivre.com.br/informatica/portateis-acessorios/notebooks/notebook_Desde_{next_offset}_NoIndex_True"
    
    self.page_count += 1
    yield scrapy.Request(url=next_page, callback=self.parse)
```

Outro aspecto importante foi a extra√ß√£o de pre√ßos. O Mercado Livre frequentemente exibe dois valores para cada produto: o pre√ßo original (riscado) e o pre√ßo promocional. Para capturar esses valores, implementei a seguinte l√≥gica:

```python
prices = product.css('span.andes-money-amount__fraction::text').getall()
```

O m√©todo `getall()` retorna uma lista com todos os elementos que correspondem ao seletor CSS especificado. No caso do Mercado Livre, este seletor captura os valores num√©ricos dos pre√ßos (sem o s√≠mbolo de moeda).

A lista `prices` normalmente cont√©m dois elementos:
- `prices[0]`: O primeiro elemento representa o pre√ßo original (old_money) - aquele que aparece riscado na interface
- `prices[1]`: O segundo elemento representa o pre√ßo promocional (new_money) - aquele que aparece como valor atual do produto

Esses valores s√£o ent√£o armazenados no dicion√°rio de resultados:

```python
yield {
    # outros campos...
    'old_money': prices[0] if len(prices) > 0 else None,
    'new_money': prices[1] if len(prices) > 1 else None
}
```

Para garantir robustez, implementei verifica√ß√µes condicionais (`if len(prices) > 0` e `if len(prices) > 1`). Isso evita erros quando um produto n√£o possui ambos os pre√ßos. Se o produto n√£o tiver pre√ßo original riscado, por exemplo, ele ter√° apenas o `new_money` sem o `old_money`.

### Processo de Transform (Transforma√ß√£o)

A transforma√ß√£o dos dados foi realizada com a biblioteca Pandas. As principais opera√ß√µes inclu√≠ram:

1. **Tratamento de dados nulos**:
   ```python
   df['old_money'] = df['old_money'].fillna('0')
   df['new_money'] = df['new_money'].fillna('0')
   df['reviews_rating_number'] = df['reviews_rating_number'].fillna('0')
   df['reviews_amount'] = df['reviews_amount'].fillna('0')
   ```

2. **Filtragem de pre√ßos** - mantendo apenas notebooks entre R$1.000 e R$10.000:
   ```python
   df = df[
       (df['old_money'] >= 1000) & (df['old_money'] <= 10000) &
       (df['new_money'] >= 1000) & (df['new_money'] <= 10000)
   ]
   ```

3. **Convers√£o de tipos de dados**:
   ```python
   df['old_money'] = df['old_money'].astype(float)
   df['new_money'] = df['new_money'].astype(float)
   df['reviews_rating_number'] = df['reviews_rating_number'].astype(float)
   df['reviews_amount'] = df['reviews_amount'].astype(int)
   ```

### Processo de Load (Carregamento)

O carregamento dos dados foi feito com SQLite:

```python
conn = sqlite3.connect('data/meli.sql')
df.to_sql('notebook', conn, if_exists='replace', index=False)
```

Ap√≥s o carregamento, realizei testes de conex√£o utilizando DBeaver, confirmando o sucesso da opera√ß√£o com uma simples consulta:

```sql
SELECT * FROM notebook
```

### Dashboard com Streamlit

Desenvolvi um dashboard interativo com Streamlit que apresenta:
- KPIs principais: total de notebooks, marcas √∫nicas e pre√ßo m√©dio
- Gr√°ficos de barras das marcas mais frequentes
- An√°lise de pre√ßo m√©dio por marca
- Satisfa√ß√£o m√©dia por marca baseada nas avalia√ß√µes

## Estrutura do Projeto

```
üì¶
‚îú‚îÄ‚îÄ data                 # Cont√©m os dados coletados e o banco SQLite
‚îÇ   ‚îî‚îÄ‚îÄ data.jsonl       # Dados extra√≠dos em formato JSONL
‚îÇ   ‚îî‚îÄ‚îÄ meli.sql         # Banco de dados SQLite com dados processados
‚îú‚îÄ‚îÄ src                  # C√≥digo-fonte do projeto
‚îÇ   ‚îú‚îÄ‚îÄ extraction       # Scripts de coleta de dados (Scrapy)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ coleta       # Projeto Scrapy
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ spiders  # Spiders para extra√ß√£o de dados
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ notebook.py # Spider espec√≠fico para notebooks
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ items.py        # Defini√ß√£o dos itens a serem coletados
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ settings.py     # Configura√ß√µes do Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ transformLoad    # Scripts de processamento e carregamento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py      # Script de ETL para tratar os dados coletados
‚îÇ   ‚îî‚îÄ‚îÄ dashboard        # C√≥digo da interface de visualiza√ß√£o
‚îÇ       ‚îî‚îÄ‚îÄ app.py       # Aplica√ß√£o Streamlit para visualiza√ß√£o dos dados
‚îî‚îÄ‚îÄ README.md            # Este arquivo
```

## Como Usar

### Pr√©-requisitos

- Python 3.8+
- pip (gerenciador de pacotes do Python)
- Bibliotecas: Scrapy, Pandas, SQLite3, Streamlit

### Instala√ß√£o

```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/meli-market-research.git
cd meli-market-research

# Instale as depend√™ncias
pip install -r requirements.txt
```

### Executando o Projeto

#### 1. Extra√ß√£o de Dados (Coleta)

```bash
cd src/extraction
scrapy crawl notebook -o ../../data/data.jsonl
```

Este comando inicia o spider que coleta informa√ß√µes de at√© 10 p√°ginas de resultados de notebooks no Mercado Livre.

#### 2. Transforma√ß√£o e Carregamento dos Dados

```bash
cd ../transformLoad
python main.py
```

Este script limpa os dados, remove valores inv√°lidos, formata os pre√ßos corretamente e carrega tudo em um banco de dados SQLite.

#### 3. Visualiza√ß√£o dos Dados

```bash
cd ../dashboard
streamlit run app.py
```

Este comando inicia o dashboard Streamlit que exibe graficamente os insights extra√≠dos dos dados.

## Insights Dispon√≠veis

O dashboard fornece as seguintes informa√ß√µes:

- Total de notebooks analisados
- N√∫mero de marcas dispon√≠veis
- Pre√ßo m√©dio dos notebooks
- Ranking das marcas mais populares
- Compara√ß√£o de pre√ßos m√©dios por marca
- Satisfa√ß√£o dos clientes por marca (avalia√ß√µes)

## Tecnologias Utilizadas

- **Scrapy**: Framework para extra√ß√£o de dados
- **Pandas**: Biblioteca para manipula√ß√£o e an√°lise de dados
- **SQLite**: Sistema de gerenciamento de banco de dados relacional
- **DBeaver**: Ferramenta universal de banco de dados para checagem dos dados e teste de conex√£o 
- **Streamlit**: Biblioteca para cria√ß√£o de aplica√ß√µes web interativas

## Poss√≠veis Melhorias Futuras

- Implementar coleta autom√°tica agendada
- Adicionar an√°lise de sentimento das avalia√ß√µes
- Incluir compara√ß√£o hist√≥rica de pre√ßos
- Expandir para outras categorias de produtos
- Implementar alertas de pre√ßo

## Licen√ßa

Este projeto est√° licenciado sob a licen√ßa MIT - veja o arquivo LICENSE para detalhes.

## Autor

Filipe Albuquerque - [filipi_98@hotmail.com](mailto:filipi_98@hotmail.com)

---

‚≠êÔ∏è Se este projeto te ajudou, considere dar uma estrela! ‚≠êÔ∏è
