# MELI Notebook Market Research

Um projeto completo de an√°lise de mercado que coleta, processa e visualiza dados sobre notebooks no Mercado Livre Brasil.

## Problema de Neg√≥cio

A Samsung necessita de uma pesquisa de mercado aprofundada na categoria de notebooks para tomar decis√µes estrat√©gicas. A empresa deseja:

* Realizar uma an√°lise completa da concorr√™ncia de notebooks em marketplaces, especificamente no Mercado Livre
* Coletar dados estruturados (Nome, Marca, Pre√ßo, Avalia√ß√µes, Vendedor, etc.)
* Desenvolver um sistema de WebScraping para obten√ß√£o automatizada dos dados
* Armazenar as informa√ß√µes em um banco de dados SQL para consultas futuras
* Criar um dashboard interativo para visualiza√ß√£o e an√°lise dos insights obtidos

Este projeto √© uma solu√ß√£o end-to-end de webscraping, seguindo o formato Extract, Transform e Load (ETL).

### Processo de Extract (Extra√ß√£o)

Utilizei o framework Scrapy para webscraping. O spider `notebook.py` foi programado para navegar pela p√°gina de notebooks do Mercado Livre e extrair informa√ß√µes detalhadas.

Um dos maiores desafios foi compreender o padr√£o de URLs do Mercado Livre. Identifiquei que a navega√ß√£o seguia um padr√£o espec√≠fico:
- Para a segunda p√°gina: `.../notebook_Desde_49_NoIndex_True`
- Para as p√°ginas seguintes: `.../notebook_Desde_{49+(p√°gina-1)*48}_NoIndex_True`

Este padr√£o est√° implementado no bloco condicional ao final do spider, permitindo a navega√ß√£o autom√°tica entre as p√°ginas:

```python
if self.page_count < self.max_page:
    # Verificamos se o contador de p√°ginas est√° abaixo do limite m√°ximo definido (10 p√°ginas)
    if self.page_count == 1:
        # Tratamento especial para a segunda p√°gina (depois da primeira)
        # A segunda p√°gina sempre usa um offset fixo de 49 produtos
        next_page = "https://lista.mercadolivre.com.br/informatica/portateis-acessorios/notebooks/notebook_Desde_49_NoIndex_True"
    else:
        # Para as p√°ginas subsequentes (terceira em diante)
        # Calculamos o offset com base em uma f√≥rmula identificada no padr√£o do site:
        # - Primeira p√°gina: sem offset
        # - Segunda p√°gina: offset de 49
        # - Terceira p√°gina em diante: 49 + (n√∫mero_da_p√°gina - 1) * 48
        next_offset = 49 + (self.page_count - 1) * 48
        next_page = f"https://lista.mercadolivre.com.br/informatica/portateis-acessorios/notebooks/notebook_Desde_{next_offset}_NoIndex_True"
    
    # Incrementamos o contador de p√°ginas para a pr√≥xima itera√ß√£o
    self.page_count += 1
    
    # Criamos uma nova requisi√ß√£o para a pr√≥xima p√°gina e a adicionamos √† fila do Scrapy
    # O callback self.parse faz com que a mesma fun√ß√£o seja chamada para processar os resultados
    yield scrapy.Request(url=next_page, callback=self.parse)
```

Este sistema funciona porque:
1. Inicializamos `page_count = 1` e `max_page = 10` no in√≠cio da classe
2. A condi√ß√£o `if self.page_count < self.max_page` limita a coleta a no m√°ximo 10 p√°ginas
3. Tratamos a primeira transi√ß√£o (p√°gina 1 ‚Üí p√°gina 2) de forma especial, pois o Mercado Livre usa um offset fixo de 49
4. Para as pr√≥ximas p√°ginas, calculamos o offset dinamicamente baseado no padr√£o descoberto
5. Incrementamos o contador ap√≥s cada p√°gina processada
6. Usamos o `yield` para gerar uma nova requisi√ß√£o que ser√° processada pelo Scrapy

Outro aspecto importante foi a extra√ß√£o de pre√ßos. O Mercado Livre frequentemente exibe dois valores: o pre√ßo original (`old_money`) e o pre√ßo promocional (`new_money`). Extra√≠ esses valores usando:

```python
prices = product.css('span.andes-money-amount__fraction::text').getall()
'old_money': prices[0] if len(prices) > 0 else None,
'new_money': prices[1] if len(prices) > 1 else None
```

### Processo de Transform (Transforma√ß√£o)

A transforma√ß√£o dos dados foi realizada com a biblioteca Pandas. As principais opera√ß√µes inclu√≠ram:

1. **Tratamento de dados nulos**:
   ```python
   df['old_money'] = df['old_money'].fillna('0')
   df['new_money'] = df['new_money'].fillna('0')
   df['reviews_rating_number'] = df['reviews_rating_number'].fillna('0')
   df['reviews_amount'] = df['reviews_amount'].fillna('0')
   ```

2. **Filtragem de pre√ßos** - mantendo apenas notebooks entre R$1.000 e R$10.000:
   ```python
   df = df[
       (df['old_money'] >= 1000) & (df['old_money'] <= 10000) &
       (df['new_money'] >= 1000) & (df['new_money'] <= 10000)
   ]
   ```

3. **Convers√£o de tipos de dados**:
   ```python
   df['old_money'] = df['old_money'].astype(float)
   df['new_money'] = df['new_money'].astype(float)
   df['reviews_rating_number'] = df['reviews_rating_number'].astype(float)
   df['reviews_amount'] = df['reviews_amount'].astype(int)
   ```

### Processo de Load (Carregamento)

O carregamento dos dados foi feito com SQLite:

```python
conn = sqlite3.connect('data/meli.sql')
df.to_sql('notebook', conn, if_exists='replace', index=False)
```

#### Teste de Conex√£o com DBeaver

Ap√≥s o carregamento dos dados, realizei testes de conex√£o utilizando o DBeaver para verificar se o banco de dados foi criado corretamente e se a conex√£o poderia ser estabelecida.

![Teste de Conex√£o com DBeaver](images/teste_conexao.png)
*Figura 1: Configura√ß√£o da conex√£o com o banco de dados SQLite no DBeaver.*

#### Checagem dos Dados no DBeaver

Para validar a integridade dos dados carregados, executei uma consulta simples no DBeaver:

```sql
SELECT * FROM notebook
```

Esta consulta retornou todos os registros da tabela, permitindo verificar se os dados foram corretamente carregados e estruturados.

![Checagem dos Dados no DBeaver](images/checagem_dados.png)
*Figura 2: Resultados da consulta SQL exibindo os dados coletados dos notebooks.*

### Dashboard com Streamlit

Desenvolvi um dashboard interativo com Streamlit que apresenta:
- KPIs principais: total de notebooks, marcas √∫nicas e pre√ßo m√©dio
- Gr√°ficos de barras das marcas mais frequentes
- An√°lise de pre√ßo m√©dio por marca
- Satisfa√ß√£o m√©dia por marca baseada nas avalia√ß√µes

## Estrutura do Projeto

```
üì¶
‚îú‚îÄ‚îÄ data                 # Cont√©m os dados coletados e o banco SQLite
‚îÇ   ‚îî‚îÄ‚îÄ data.jsonl       # Dados extra√≠dos em formato JSONL
‚îÇ   ‚îî‚îÄ‚îÄ meli.sql         # Banco de dados SQLite com dados processados
‚îú‚îÄ‚îÄ src                  # C√≥digo-fonte do projeto
‚îÇ   ‚îú‚îÄ‚îÄ extraction       # Scripts de coleta de dados (Scrapy)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ coleta       # Projeto Scrapy
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ spiders  # Spiders para extra√ß√£o de dados
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ notebook.py # Spider espec√≠fico para notebooks
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ items.py        # Defini√ß√£o dos itens a serem coletados
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ settings.py     # Configura√ß√µes do Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ transformLoad    # Scripts de processamento e carregamento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py      # Script de ETL para tratar os dados coletados
‚îÇ   ‚îî‚îÄ‚îÄ dashboard        # C√≥digo da interface de visualiza√ß√£o
‚îÇ       ‚îî‚îÄ‚îÄ app.py       # Aplica√ß√£o Streamlit para visualiza√ß√£o dos dados
‚îî‚îÄ‚îÄ README.md            # Este arquivo
```

## Como Usar

### Pr√©-requisitos

- Python 3.8+
- pip (gerenciador de pacotes do Python)
- Bibliotecas: Scrapy, Pandas, SQLite3, Streamlit

### Instala√ß√£o

```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/meli-market-research.git
cd meli-market-research

# Instale as depend√™ncias
pip install -r requirements.txt
```

### Executando o Projeto

#### 1. Extra√ß√£o de Dados (Coleta)

```bash
cd src/extraction
scrapy crawl notebook -o ../../data/data.jsonl
```

Este comando inicia o spider que coleta informa√ß√µes de at√© 10 p√°ginas de resultados de notebooks no Mercado Livre.

#### 2. Transforma√ß√£o e Carregamento dos Dados

```bash
cd ../transformLoad
python main.py
```

Este script limpa os dados, remove valores inv√°lidos, formata os pre√ßos corretamente e carrega tudo em um banco de dados SQLite.

#### 3. Visualiza√ß√£o dos Dados

```bash
cd ../dashboard
streamlit run app.py
```

Este comando inicia o dashboard Streamlit que exibe graficamente os insights extra√≠dos dos dados.

## Insights Dispon√≠veis

O dashboard fornece as seguintes informa√ß√µes:

- Total de notebooks analisados
- N√∫mero de marcas dispon√≠veis
- Pre√ßo m√©dio dos notebooks
- Ranking das marcas mais populares
- Compara√ß√£o de pre√ßos m√©dios por marca
- Satisfa√ß√£o dos clientes por marca (avalia√ß√µes)

## Tecnologias Utilizadas

- **Scrapy**: Framework para extra√ß√£o de dados
- **Pandas**: Biblioteca para manipula√ß√£o e an√°lise de dados
- **SQLite**: Sistema de gerenciamento de banco de dados relacional
- **DBeaver**: Ferramenta universal de banco de dados para checagem dos dados e teste de conex√£o 
- **Streamlit**: Biblioteca para cria√ß√£o de aplica√ß√µes web interativas

## Poss√≠veis Melhorias Futuras

- Implementar coleta autom√°tica agendada
- Adicionar an√°lise de sentimento das avalia√ß√µes
- Incluir compara√ß√£o hist√≥rica de pre√ßos
- Expandir para outras categorias de produtos
- Implementar alertas de pre√ßo

## Licen√ßa

Este projeto est√° licenciado sob a licen√ßa MIT - veja o arquivo LICENSE para detalhes.

## Autor

Filipe Albuquerque - [filipi_98@hotmail.com](mailto:filipi_98@hotmail.com)

---

‚≠êÔ∏è Se este projeto te ajudou, considere dar uma estrela! ‚≠êÔ∏è
